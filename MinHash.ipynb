{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinHash on HT books\n",
    "\n",
    "[MinHash](https://en.wikipedia.org/wiki/MinHash) is a technique for estimating similarity of two sets quickly. Where a regular hash can fingerprint completely identical objects, MinHash is particularly useful for comparing sets that are similar but inexactly. It's used often for texts, where the *set* is comprised of the words in the text.\n",
    "\n",
    "MinHash scales linearly, making it useful for an initial scan of similar texts before using a more computationally intensive algorithm.\n",
    "\n",
    "This project uses `datasketch` to convert HTRC Extracted Features files to MinHashes, using a basic serialization method to write the HathiTrust volume ID and hash to file.\n",
    "\n",
    "## Serializing Hashes\n",
    "\n",
    "The basic function for hashing a text is `make_hash(htid, wordset)`, available in `hash_utils.py`. It takes a string name for the volume (max 30 characters) and a set of words. Currently, the set of words is unweighted by frequency. Other keyword arguments are passed on to `datasketch`, most importantant the `num_perm` parameter that allows you to increase the size of the hash for better estimation at a cost of more memory and time. You can also change the `seed` for hashing.\n",
    "\n",
    "`make_hash` returns a byte serialization of the text, using the following format:\n",
    "\n",
    "- seed (8bytes long long)\n",
    "- length of hash (4 bytes),\n",
    "- volume name (max 30 char string, padded with `\\x00`)\n",
    "- hashes (4 bytes each, * length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x80\\x00\\x00\\x00testid\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xacPG\\x182\\xd8$%\\ny\\x02\\x17k\\xaf\\xacI\\x15\\xef\\x1e\\x08\\xdf~\\x1e<\\xe5\\xbf\\xda\\nGP\\xd5\\x120\\xbc\\x1c\\'\\xf2\\xbb^\\x05Q\\xee\\x0e\\x08\\x00\\xddl\\x04\\x1d\\xe4\\xb8\\x14\\xfc:x6u\\x1e#B\\x92\\xb9N\\x06\\x15\"\\xb1)j\\x13\\x7f&\\xba\\x90~8q\\\\\\xdcJ\\xce\\x0e\\xac-Q\\x8e\\xbe\\x02\\x97\\xb6?\\x03\\x02Q\\xb0\\x05\\xe3+\\xfd9$\\xf3e#\\x9fh\\x10\\r\\x8c\\xa7j\\x13G\\x12=#$P\\xf4Q\\x16a\\xe3\\x0c\\x17\\xef9\\x19\\x90\\xc4\\xe5\\x0f\\x03d\\xdb\"<\\x16\\x01&\\xe4dm\\x06\\x9e\\xb8\\xa8,\\x1a\\xb001\\xa4\\xce\\x8c\\x19(\\x84\\x97\\x0bE1H\\x1bkK]\\x02\"-\\xcea\\x81\\xedr\\x07\\x1a\\xb7\\x02\\x0f\\xd7\\xb6\\xd2\\n\\xc1\\x7f\\x00\\r\\xd2~`\\x16\\x17\\xf4<\\x172\\x03g\\r\\xae\\xd9\\x88n\\x9a\\x1f\\xb4d\\x1a\\xf4\\xa9j\\xda3.!vT\\xd9\\x05\\xcd\\xbf`\\x1f4\\x86\\xb2\\n\\xcd7\\'\\x1c\\x8f^\\xcc0G\\tT>\\xf2_J\\x1di\\xd6m+\\x87\\xc4d%\\xafD\\x8bKju\\xcb\\'\\xde=z$\\xfcH/.\\x98\\x974\\x00+|.\\t\\xb6\\xce\\xa9\\x00\\t6\\xc0\\x18o\\x1b\\xbd3=\\x14\\x8c6\\xc9\\x997\\x10C\\x89m\\x1a,\\x95\\x0f5\\xf2]\\xe6\\x1f\\xc3\\x1d\\'&\\x18A\\xb1EL\\xaaV\\x12*\\x16;\\x82\\xe0\\xaa1\\x817\\xc2\\xc2\\x1f\\xc8\\x05:8\\xdd\\xdf\\x94i\\x04_\\xb2u\\xc6fw\\x1eu\\x02\\x8f\\x08\\x01\\x8f\\xd6\\x01bb\\xd2\\x08\\xab\\xa1\\xc9\\x17\\x89\\x80\\x812d\\xf3\\x12/i\\x97\\xc3\\x1b\\xc8\\xd9\\xc3\\x0f\\xf4\\xccO\\x00)\\x00B\\x16\\x92\\xdf\\x98\\x02|\\xddj\\x1d{L\\xed\\x01\\x1e\\xb9&\\x13\\xd1\\x8a\\x8dD\\x85\\xd2\\xd6\\x03=\\xb2\\xb1%\\xa95\\xda\\x0fH\\xb7\\xdc\\x1b\\xd1\\x99\\xaa\\x18\\xf2>\\xe48<\\xa9\\x9c?:K\\x9d\\x05\\xda\\x96o\\x1f\\x94/\\xae fd\\x08E\\xae;\\x9e\\x0b\\xee\\xf1f\\x85\\xcdf*\\x03\\xc4\\xd5\\xda\\x86\\xcc\\xc5u\\x06u\\xde`\\x02\\xc1/<j\\xf7\\x15\\x06\\x16q\\xdc\\xfe\\n.\\x0e\\xb9\\x10\\x03\\xe0H\\x1fMX\\xda\\x14\\xc8j\\xe6\\x80\\xa1,j\\x10\\xe3\\x89\\x84\\x07'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hash_utils import make_hash\n",
    "h = make_hash('testid', {'it', 'was', 'the', 'best', 'of', 'times'})\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crunching Hashes (Method 1: Pre-processed files)\n",
    "\n",
    "For my input, I use a pre-crunched version of the Extracted Features dataset created for HTRC+Bookworm, documented [here](https://github.com/htrc/HTRC-Useful-Datasets/tree/master/volume-tables). Those files are tables of volumeid/token/count for 13m books and are simply used for convenience: you can request them from HTRC or simply process from the original dataset (see below).\n",
    "\n",
    "The code for using those files is in [MakeMinHash.py](./MakeMinHash.py). It runs a single process, transforming an input H5 file into a custom data file. To parallelize, I use `GNU Parallel` on the command line. e.g.\n",
    "\n",
    "```bash\n",
    "find ../datasets/volume-tables/data/. -name '*h5' | parallel -j20 -n1 -u python MakeMinHash.py {} data/minhashes/{/.}.dat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crunching Hashes (Method 2: Extracted Features Files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for converting Extracted Features files to Minhashes is in [./MakeMinHashFromIDs.py](./MakeMinHashFromIDs.py). Below is the usage doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: MakeMinHashFromIDs.py [-h]\n",
      "                             (--ids IDS [IDS ...] | --paths PATHS [PATHS ...])\n",
      "                             [--num_perm NUM_PERM]\n",
      "                             outdir\n",
      "\n",
      "Process a BW file into a Minhash and save using a succinct serialization\n",
      "format.\n",
      "\n",
      "positional arguments:\n",
      "  outdir                Directory to save Minhashes. File name is\n",
      "                        hashes.{RANDINT}.dat.\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --ids IDS [IDS ...]   HathiTrust IDs to convert to MinHash. Mutually\n",
      "                        exclusive with --paths.\n",
      "  --paths PATHS [PATHS ...]\n",
      "                        Paths to HathiTrust EF file to convert to MinHash.\n",
      "                        Mutually exclusive with --ids.\n",
      "  --num_perm NUM_PERM   Number of permutations for MinHash. Defaults to 128\n"
     ]
    }
   ],
   "source": [
    "!python MakeMinHashFromIDs.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deserializing Hashes\n",
    "\n",
    "Use `deserialize_lm`. Even though the length is saved in the serialization, this basic function asks for the length of the hash. This is identical to the `num_perm` argument, `128` if the default wasn't changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'testid', <datasketch.lean_minhash.LeanMinHash at 0x7f83d6134708>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hash_utils import deserialize_lm\n",
    "volid, minhash = deserialize_lm(h, 128)\n",
    "volid, minhash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, I save hashes to a file with the same settings, so I simply peek at the first 12 bytes of the file to get the length, then read the rest with the assumption that it is all constant-length hashes. If following the same usecase, use `HashReader`. e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'uva.x030509704' <datasketch.lean_minhash.LeanMinHash object at 0x7f84485db678>\n",
      "b'uva.x030509706' <datasketch.lean_minhash.LeanMinHash object at 0x7f84485db3a8>\n",
      "b'uva.x030509708' <datasketch.lean_minhash.LeanMinHash object at 0x7f84485db318>\n",
      "b'uva.x030509709' <datasketch.lean_minhash.LeanMinHash object at 0x7f84485db1f8>\n",
      "b'uva.x030509710' <datasketch.lean_minhash.LeanMinHash object at 0x7f84485db288>\n"
     ]
    }
   ],
   "source": [
    "from hash_utils import HashReader\n",
    "example_in = 'data/minhashes/counts.64.dat'\n",
    "\n",
    "with HashReader(example_in) as hr:\n",
    "    i = 0\n",
    "    for htid, minhash in hr.hashes():\n",
    "        print(htid, minhash)\n",
    "        i += 1\n",
    "        if i >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the hashes\n",
    "\n",
    "The MinHashes can be used for comparison of sets using locality-sensitive hashing, as per [`datasketch` docs](https://ekzhu.github.io/datasketch/lsh.html). `MinHashLSH` needs a threshold for 'similarity' at initialization, while `MinHashLSHForest` allows a 'top *n*' query for finding a number of similar texts. \n",
    "\n",
    "e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6551 volumes added with 28 errors\n",
      "CPU times: user 932 ms, sys: 8 ms, total: 940 ms\n",
      "Wall time: 940 ms\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHashLSHForest\n",
    "lshf = MinHashLSHForest(num_perm=128)\n",
    "\n",
    "errs, added = 0, 0\n",
    "with HashReader(example_in) as hr:\n",
    "    for htid, minhash in hr.hashes():\n",
    "        try:\n",
    "            lshf.add(htid, minhash)\n",
    "            added += 1\n",
    "        except:\n",
    "            errs += 1\n",
    "print(added, 'volumes added with', errs, 'errors')\n",
    "lshf.index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error catching here is a quirk of my speed-optimized workflow: some files may have have been hashed twice (likely with two incomplete fingerprints). In my case, I just ignore those particular hashes and rehash from the original dataset file, but a more delicate hashing process wouldn't need to do that.\n",
    "\n",
    "Note that MinHashLSHForest needed a call to `index` at the end. Then, you can provide a hash query to find similar texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'coo.31924069731796' <datasketch.lean_minhash.LeanMinHash object at 0x7f83d60bc3a8>\n",
      "Closest 10 volumes (unsorted) are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[b'uc1.b4017365',\n",
       " b'uc1.b5173966',\n",
       " b'coo.31924069731762',\n",
       " b'coo.31924069731796',\n",
       " b'coo.31924069731770',\n",
       " b'uc1.b4450359',\n",
       " b'coo.31924069731788',\n",
       " b'uc1.b4498285',\n",
       " b'uc1.b3810741',\n",
       " b'coo.31924069731705']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the last minhash from the loop for a query\n",
    "print(htid, minhash)\n",
    "print('Closest 10 volumes (unsorted) are:')\n",
    "lshf.query(minhash, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
