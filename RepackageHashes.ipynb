{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repackaging MinHashes\n",
    "\n",
    "The process of creating MinHashes from Bookworm input files led to some duplicates. Since I'm not sure if duplicated volumes are fingerprints of the entire text or if they've been accidentally fragmented, I reprocess those volumes from the original [Extracted Features](https://wiki.htrc.illinois.edu/display/COM/Extracted+Features+Dataset) files. It's only a small portion of the collection. At the bottom, this notebook repackages the initial MinHashes into more evenly sized data files, without the original versions of the duplicated volumes.\n",
    "\n",
    "This notebook is just for posterity: it's unlikely to be useful outside of my use case.\n",
    "\n",
    "## Checking for volumes that need to be reprocessed\n",
    "\n",
    "Volumes that show up more than once are saved to `duplicates.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000,1000000,1500000,2000000,2500000,3000000,3500000,4000000,4500000,5000000,5500000,6000000,6500000,7000000,7500000,8000000,8500000,9000000,9500000,10000000,10500000,11000000,11500000,12000000,12500000,13000000,13500000,13751529\n",
      "CPU times: user 8min 34s, sys: 7.77 s, total: 8min 42s\n",
      "Wall time: 14min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os\n",
    "from hash_utils import HashReader\n",
    "import glob\n",
    "\n",
    "datpaths = glob.glob('data/minhashes/*.dat')\n",
    "all_ids = []\n",
    "i = 0\n",
    "\n",
    "for path in datpaths:\n",
    "    with HashReader(path) as hr:\n",
    "        filename = os.path.splitext(os.path.split(path)[-1])[0]\n",
    "        for htid, minhash in hr.hashes():\n",
    "            all_ids.append(htid)\n",
    "            i += 1\n",
    "            if i % 500000 == 0:\n",
    "                print(i, end=',')\n",
    "print(i)\n",
    "counts = pd.Series(all_ids).value_counts()\n",
    "dupes = counts[counts > 1].index.tolist()\n",
    "\n",
    "with open('duplicates.txt', mode='w') as f:\n",
    "    f.write(\"\\n\".join(dupes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import utils\n",
    "successes, fails = [], []\n",
    "for htid in dupes:\n",
    "    path = '/notebooks/features/' + utils.id_to_rsync(htid)\n",
    "    try:\n",
    "        os.stat(path)\n",
    "        successes.append(path)\n",
    "    except:\n",
    "        fails.append(htid)\n",
    "        \n",
    "with open('success-paths.txt', mode='w') as f:\n",
    "    f.write(\"\\n\".join(successes))\n",
    "    \n",
    "with open('fail-ids.txt', mode='w') as f:\n",
    "    f.write(\"\\n\".join(fails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repackage MinHashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite all non-duplicated hashes. A few minutes of extra processing in the service of a cleaner dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292080 ['data/minhashes-repack/minhashes-2.0.dat']\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "\n",
    "with open('duplicates.txt', mode='r') as f:\n",
    "    dupes = set(f.read().split('\\n'))\n",
    "dupeset = set(dupes)\n",
    "\n",
    "i, j = 0, 0\n",
    "\n",
    "f = open('data/minhashes-repack/minhashes-3.%d.dat' % j, mode='wb')\n",
    "for path in datpaths:\n",
    "    with HashReader(path) as hr:\n",
    "        for htid, part in hr.hashes(deserialize=False):\n",
    "            if htid in dupeset:\n",
    "                continue\n",
    "            f.write(part)\n",
    "            i += 1\n",
    "            if i % 500000 == 0:\n",
    "                print(i, end=',')\n",
    "            if i % 2000000 == 0:\n",
    "                j += 1\n",
    "                f.close()\n",
    "                f = open('data/minhashes-repack/minhashes-3.%d.dat' % j, mode='wb')\n",
    "\n",
    "f.close()\n",
    "print(i, glob.glob('data/minhashes-repack/minhashes-3*'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
